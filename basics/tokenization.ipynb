{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It seems that the code is attempting to import a module named inflect, but it's unable to find it because the module is not installed in your Python environment.\",\n",
       " 'The inflect module is typically used for converting numbers into words, pluralizing and singularizing words, etc.',\n",
       " 'To resolve this issue, you can install the inflect module using pip:']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph=\"\"\"It seems that the code is attempting to import a module named inflect, but it's unable to find it because the module is not installed in your Python environment.\n",
    "\n",
    "The inflect module is typically used for converting numbers into words, pluralizing and singularizing words, etc.\n",
    "\n",
    "To resolve this issue, you can install the inflect module using pip:\"\"\"\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'seems',\n",
       " 'that',\n",
       " 'the',\n",
       " 'code',\n",
       " 'is',\n",
       " 'attempting',\n",
       " 'to',\n",
       " 'import',\n",
       " 'a',\n",
       " 'module',\n",
       " 'named',\n",
       " 'inflect',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'unable',\n",
       " 'to',\n",
       " 'find',\n",
       " 'it',\n",
       " 'because',\n",
       " 'the',\n",
       " 'module',\n",
       " 'is',\n",
       " 'not',\n",
       " 'installed',\n",
       " 'in',\n",
       " 'your',\n",
       " 'Python',\n",
       " 'environment',\n",
       " '.',\n",
       " 'The',\n",
       " 'inflect',\n",
       " 'module',\n",
       " 'is',\n",
       " 'typically',\n",
       " 'used',\n",
       " 'for',\n",
       " 'converting',\n",
       " 'numbers',\n",
       " 'into',\n",
       " 'words',\n",
       " ',',\n",
       " 'pluralizing',\n",
       " 'and',\n",
       " 'singularizing',\n",
       " 'words',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'To',\n",
       " 'resolve',\n",
       " 'this',\n",
       " 'issue',\n",
       " ',',\n",
       " 'you',\n",
       " 'can',\n",
       " 'install',\n",
       " 'the',\n",
       " 'inflect',\n",
       " 'module',\n",
       " 'using',\n",
       " 'pip',\n",
       " ':']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=nltk.word_tokenize(paragraph)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'helps', 'break', 'down', 'text', 'into', 'smaller', 'units', '.']\n",
      "\n",
      "Sentence Tokenization:\n",
      "['Tokenization is an important step in natural language processing.', 'It helps break down text into smaller units.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# Sample text\n",
    "text = \"Tokenization is an important step in natural language processing. It helps break down text into smaller units.\"\n",
    "\n",
    "# Word tokenization\n",
    "words = nltk.word_tokenize(text)\n",
    "print(\"Word Tokenization:\")\n",
    "print(words)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences =nltk.sent_tokenize(text)\n",
    "print(\"\\nSentence Tokenization:\")\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['stem', 'and', 'lemmat', 'are', 'import', 'techniqu', 'for', 'text', 'normal', 'in', 'nlp', '.']\n",
      "Lemmatized words: ['Stemming', 'and', 'lemmatization', 'are', 'important', 'technique', 'for', 'text', 'normalization', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Stemming and lemmatization are important techniques for text normalization in NLP.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags: [('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('tagging', 'NN'), ('helps', 'VBZ'), ('in', 'IN'), ('understanding', 'VBG'), ('the', 'DT'), ('syntactic', 'JJ'), ('structure', 'NN'), ('of', 'IN'), ('sentences', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"Parts of speech tagging helps in understanding the syntactic structure of sentences.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"POS tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\dell\\anaconda3\\lib\\site-packages (7.0.0)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from inflect) (1.10.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\lib\\site-packages (from inflect) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number in words: twelve thousand, three hundred and forty-five\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "\n",
    "# Create an instance of the inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "# Convert a number into words\n",
    "number = 12345\n",
    "words = p.number_to_words(number)\n",
    "\n",
    "print(\"Number in words:\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'cats', 'better', 'quickly']\n",
      "POS Tags: ['VBG', 'NNS', 'RBR', 'RB']\n",
      "Lemmatized Words: ['run', 'cat', 'well', 'quickly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create WordNetLemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize a word with specified part-of-speech\n",
    "def lemmatize_word(word, pos):\n",
    "    if pos.startswith('J'):\n",
    "        pos = wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        pos = wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        pos = wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        pos = wordnet.ADV\n",
    "    else:\n",
    "        pos = wordnet.NOUN  # Default to noun if part-of-speech is not recognized\n",
    "    \n",
    "    return lemmatizer.lemmatize(word, pos=pos)\n",
    "\n",
    "# Function to lemmatize a list of words with their corresponding part-of-speech tags\n",
    "def lemmatize_words(words, pos_tags):\n",
    "    # Zip the words and pos_tags lists together and lemmatize each word\n",
    "    lemmatized_words = [lemmatize_word(word, pos) for word, pos in zip(words, pos_tags)]\n",
    "    return lemmatized_words\n",
    "\n",
    "# Example usage\n",
    "words = ['running', 'cats', 'better', 'quickly']\n",
    "pos_tags = ['VBG', 'NNS', 'RBR', 'RB']  # Verb, plural noun, comparative adjective, adverb\n",
    "\n",
    "# Lemmatize the list of words with their corresponding part-of-speech tags\n",
    "lemmatized_words = lemmatize_words(words, pos_tags)\n",
    "print(\"Original Words:\", words)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
